import osmnx as ox
import networkx as nx
import geopandas as gpd
import pandas as pd
import json
import random
import itertools
import os
import math
from collections import deque

import argparse

# --- Configuration ---
EDGES_FILE = "data/walking_network_edges.geojson"
NODES_FILE = "data/walking_network_nodes.geojson"
OVERRIDE_NODES_FILE = "data/graph_overrides_nodes.geojson"
OVERRIDE_EDGES_FILE = "data/graph_overrides_edges.geojson"
POINTS_FILE = "data/selected_points.json" # Expected location of POIs and Entrances
DP_METADATA_FILE = "data/decision_points_metadata.json" # Generated by Isovist tool
THRESHOLD_PERCENT = 0.16 # 16%: keep candidates with L(p_k) <= (1 + THRESHOLD_PERCENT) * L(p_1)
DEFAULT_KAPPA = 1.0  # User-defined: if importance is unspecified, treat all pairs equally.

# --- CLI Arguments ---
parser = argparse.ArgumentParser(description='Stage A Optimization')
parser.add_argument('--w_local_length', type=float, default=1.0, help='Weight for local path length')
parser.add_argument('--w_local_nodes', type=float, default=1.0, help='Weight for local decision points')
parser.add_argument('--w_local_angle', type=float, default=10.0, help='Weight for local turning angle')
parser.add_argument('--w_global_length', type=float, default=5.0, help='Weight for global unique edge length')
parser.add_argument('--w_global_nodes', type=float, default=5.0, help='Weight for global unique nodes')
args = parser.parse_args()

# --- Load Decision Point Metadata ---
valid_decision_points = set()
if os.path.exists(DP_METADATA_FILE):
    print(f"Loading decision points from {DP_METADATA_FILE}...")
    with open(DP_METADATA_FILE, 'r') as f:
        dp_data = json.load(f)
        for dp in dp_data.get("decision_points", []):
            if dp.get("is_decision_point"):
                # Ensure ID is consistent (int vs string)
                try:
                    valid_decision_points.add(int(dp["id"]))
                except:
                    valid_decision_points.add(dp["id"])
    print(f"Loaded {len(valid_decision_points)} valid decision points.")
else:
    print("No decision point metadata found. Fallback: all nodes count as decision points.")

# --- Stage A (Wayfinding Scheme Optimization) ---
# User-defined speed cap (paper does not specify a cap):
# - Set to None to disable and follow the 16% stopping rule only.
# - Set to an int (e.g., 10/25/50) to bound runtime per pair.
MAX_CANDIDATES_PER_PAIR = 25

# Paper default weights:
W_LOCAL_LENGTH = args.w_local_length
W_LOCAL_NODES = args.w_local_nodes
W_LOCAL_ANGLE = args.w_local_angle
W_GLOBAL_LENGTH = args.w_global_length
W_GLOBAL_NODES = args.w_global_nodes

print(f"Weights: LL={W_LOCAL_LENGTH}, LN={W_LOCAL_NODES}, LA={W_LOCAL_ANGLE}, GL={W_GLOBAL_LENGTH}, GN={W_GLOBAL_NODES}")

# User-defined parameters (paper does not specify temperature schedule / initialization):
SA_RANDOM_SEED = 0
SA_MAX_ITERATIONS = 50_000
SA_STOP_WINDOW = 1000
SA_STOP_REL_DELTA = 0.01  # 1% over SA_STOP_WINDOW iterations
SA_T0 = 1.0
SA_ALPHA = 0.9995  # geometric cooling
SA_TMIN = 1e-6

# User-defined: how to treat edges for global overlap + L_E normalization.
# - "undirected": treat (u,v) and (v,u) as the same edge.
# - "directed": treat (u,v) and (v,u) as different edges.
EDGE_KEY_MODE = "undirected"

# --- 1. Load Graph Data ---
print("Loading graph data...")
if not os.path.exists(EDGES_FILE) or not os.path.exists(NODES_FILE):
    # Fallback to initial generation if files missing (simplified for this context)
    print("GeoJSON files not found. Please run the previous generation step first.")
    exit()

gdf_edges = gpd.read_file(EDGES_FILE)
gdf_nodes = gpd.read_file(NODES_FILE)
gdf_override_edges = gpd.read_file(OVERRIDE_EDGES_FILE) if os.path.exists(OVERRIDE_EDGES_FILE) else None
gdf_override_nodes = gpd.read_file(OVERRIDE_NODES_FILE) if os.path.exists(OVERRIDE_NODES_FILE) else None

def _safe_int(value):
    try:
        if value is None:
            return None
        if isinstance(value, bool):
            return int(value)
        if isinstance(value, (int, float)) and int(value) == value:
            return int(value)
        s = str(value).strip()
        if s == "":
            return None
        return int(float(s))
    except Exception:
        return value

def _haversine_m(lat1, lon1, lat2, lon2):
    # Returns meters between 2 points (WGS84)
    import math
    r = 6371000.0
    phi1 = math.radians(lat1)
    phi2 = math.radians(lat2)
    dphi = math.radians(lat2 - lat1)
    dlambda = math.radians(lon2 - lon1)
    a = math.sin(dphi / 2.0) ** 2 + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2.0) ** 2
    return 2.0 * r * math.asin(math.sqrt(a))

def _polyline_total_and_prefix_m(coords, segment_index, t):
    # coords: [[lon,lat], ...], segment_index: int, t: float in [0,1]
    if not coords or len(coords) < 2:
        return 0.0, 0.0
    try:
        t = float(t)
    except Exception:
        t = 0.0
    if t < 0:
        t = 0.0
    if t > 1:
        t = 1.0
    try:
        segment_index = int(segment_index)
    except Exception:
        segment_index = 0

    total = 0.0
    prefix = 0.0
    for i in range(len(coords) - 1):
        lon1, lat1 = coords[i]
        lon2, lat2 = coords[i + 1]
        seg_len = _haversine_m(lat1, lon1, lat2, lon2)
        total += seg_len
        if i < segment_index:
            prefix += seg_len
        elif i == segment_index:
            prefix += seg_len * t
    if prefix < 0:
        prefix = 0.0
    if prefix > total:
        prefix = total
    return total, prefix

def _ensure_node(G, node_id, *, x=None, y=None):
    if node_id in G:
        return
    if x is None or y is None:
        G.add_node(node_id)
    else:
        G.add_node(node_id, x=x, y=y)

def _add_edge_min(G, u, v, length):
    if length is None:
        return
    try:
        length = float(length)
    except Exception:
        return
    if G.has_edge(u, v):
        if length < G[u][v].get('length', float('inf')):
            G.add_edge(u, v, length=length)
    else:
        G.add_edge(u, v, length=length)

# Reconstruct Graph
# G = ox.graph_from_gdfs(gdf_nodes, gdf_edges) # This requires specific index structure
# Manual reconstruction to ensure control over weights
# We use DiGraph instead of MultiDiGraph because nx.shortest_simple_paths 
# does not support MultiGraphs.
G = nx.DiGraph()

# Add nodes
for idx, row in gdf_nodes.iterrows():
    # GeoJSON ID might be in a column 'osmid' or the index.
    # geopandas read_file usually puts properties in columns.
    node_id = row['osmid'] if 'osmid' in row else row.name
    # Ensure ID is int if possible, matching edge u/v
    node_id = _safe_int(node_id)
    G.add_node(node_id, x=row.geometry.x, y=row.geometry.y)

# Add override nodes (e.g., negative IDs from the graph editor)
if gdf_override_nodes is not None:
    for idx, row in gdf_override_nodes.iterrows():
        node_id = row['id'] if 'id' in row else (row['osmid'] if 'osmid' in row else row.name)
        node_id = _safe_int(node_id)
        G.add_node(node_id, x=row.geometry.x, y=row.geometry.y)

# Add edges
for idx, row in gdf_edges.iterrows():
    u = row['u']
    v = row['v']
    # Ensure length is present
    length = row['length'] if 'length' in row else row.geometry.length
    
    # If edge exists, keep the shortest one (handle MultiGraph to DiGraph conversion)
    _add_edge_min(G, u, v, length)

# Add override edges (manual patches)
if gdf_override_edges is not None:
    for idx, row in gdf_override_edges.iterrows():
        u = _safe_int(row.get('u'))
        v = _safe_int(row.get('v'))
        if u is None or v is None:
            continue

        # Ensure nodes exist (fallback: create from geometry endpoints if needed)
        if (u not in G or v not in G) and getattr(row, "geometry", None) is not None:
            try:
                coords = list(row.geometry.coords)
            except Exception:
                coords = None
            if coords and len(coords) >= 2:
                (x1, y1) = coords[0]
                (x2, y2) = coords[-1]
                if u not in G:
                    _ensure_node(G, u, x=x1, y=y1)
                if v not in G:
                    _ensure_node(G, v, x=x2, y=y2)
        else:
            _ensure_node(G, u)
            _ensure_node(G, v)

        length = row.get('length')
        oneway = row.get('oneway')
        _add_edge_min(G, u, v, length)
        if oneway is False or str(oneway).strip().lower() in ("false", "0", "no", "n", ""):
            _add_edge_min(G, v, u, length)

# If patch nodes were snapped to an existing node or an edge segment, connect them into the graph.
# - snapped to node: add 0-length links between node and patch node (aliasing)
# - snapped to edge: split the referenced base edge by adding u->patch and patch->v (lengths proportional)
if os.path.exists(OVERRIDE_NODES_FILE):
    try:
        with open(OVERRIDE_NODES_FILE, "r", encoding="utf-8") as f:
            override_nodes_json = json.load(f)
        with open(EDGES_FILE, "r", encoding="utf-8") as f:
            base_edges_json = json.load(f)
        base_features = base_edges_json.get("features", [])
        for feat in override_nodes_json.get("features", []):
            props = feat.get("properties", {}) or {}
            new_id = _safe_int(props.get("id", props.get("osmid")))
            if new_id is None or new_id not in G:
                continue

            snapped_kind = props.get("snapped_kind")
            if snapped_kind == "node":
                snapped_node = _safe_int(props.get("snapped_node_id"))
                if snapped_node is None:
                    continue
                _ensure_node(G, snapped_node)
                _add_edge_min(G, snapped_node, new_id, 0.0)
                _add_edge_min(G, new_id, snapped_node, 0.0)
                continue

            if snapped_kind == "edge":
                fi = props.get("edge_feature_index")
                si = props.get("edge_segment_index")
                t = props.get("edge_segment_t")
                if fi is None:
                    continue
                try:
                    fi = int(fi)
                except Exception:
                    continue
                if fi < 0 or fi >= len(base_features):
                    continue
                edge_feat = base_features[fi]
                edge_props = edge_feat.get("properties", {}) or {}
                u = _safe_int(edge_props.get("u"))
                v = _safe_int(edge_props.get("v"))
                if u is None or v is None:
                    continue

                coords = (edge_feat.get("geometry", {}) or {}).get("coordinates")
                if not coords or len(coords) < 2:
                    continue

                edge_len = edge_props.get("length")
                try:
                    edge_len = float(edge_len)
                except Exception:
                    edge_len = None

                total_m, prefix_m = _polyline_total_and_prefix_m(coords, si, t)
                if edge_len is None:
                    edge_len = total_m
                if total_m <= 0:
                    continue
                ratio = 0.0 if total_m == 0 else (prefix_m / total_m)
                len1 = edge_len * ratio
                len2 = edge_len - len1

                _ensure_node(G, u)
                _ensure_node(G, v)
                _add_edge_min(G, u, new_id, len1)
                _add_edge_min(G, new_id, v, len2)

                # If reverse edge exists in the graph, connect patch node in reverse too
                if G.has_edge(v, u):
                    _add_edge_min(G, v, new_id, len2)
                    _add_edge_min(G, new_id, u, len1)
    except Exception as e:
        print(f"Warning: failed to apply override node snapping connections: {e}")

print(f"Graph constructed: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")
print(f"Graph type: {type(G)}")
print(f"Is MultiGraph: {G.is_multigraph()}")

def _edge_key(u, v):
    if EDGE_KEY_MODE == "undirected":
        return (u, v) if u <= v else (v, u)
    if EDGE_KEY_MODE == "directed":
        return (u, v)
    raise ValueError(f"Unsupported EDGE_KEY_MODE={EDGE_KEY_MODE!r}")

def _build_edge_length_by_key(graph):
    edge_length_by_key = {}
    for u, v, data in graph.edges(data=True):
        key = _edge_key(u, v)
        length = float(data.get("length", 0.0) or 0.0)
        if key not in edge_length_by_key:
            edge_length_by_key[key] = length
        else:
            if EDGE_KEY_MODE == "undirected":
                edge_length_by_key[key] = min(edge_length_by_key[key], length)
            else:
                edge_length_by_key[key] = length
    return edge_length_by_key

edge_length_by_key = _build_edge_length_by_key(G)
L_E = float(sum(edge_length_by_key.values()))
if L_E <= 0:
    raise ValueError("Total edge length L_E must be > 0; check edge 'length' attributes.")

print(f"Edge key mode: {EDGE_KEY_MODE}. L_E (normalization): {L_E:.2f}m")

def _turn_angle(graph, a, b, c):
    ax, ay = graph.nodes[a]["x"], graph.nodes[a]["y"]
    bx, by = graph.nodes[b]["x"], graph.nodes[b]["y"]
    cx, cy = graph.nodes[c]["x"], graph.nodes[c]["y"]

    v1x, v1y = ax - bx, ay - by
    v2x, v2y = cx - bx, cy - by
    dot = v1x * v2x + v1y * v2y
    cross = v1x * v2y - v1y * v2x
    
    # Calculate interior angle between B->A and B->C
    # Straight line = PI (180 deg), Sharp turn = near 0
    interior_angle = abs(math.atan2(cross, dot))
    
    # We want cost to be 0 for straight, high for turns.
    # Deviation = |PI - interior_angle|
    return abs(math.pi - interior_angle)

def _path_metrics(graph, path):
    edges = list(zip(path[:-1], path[1:]))
    length = 0.0
    edge_items = []
    for u, v in edges:
        key = _edge_key(u, v)
        length += float(graph[u][v].get("length", 0.0) or 0.0)
        edge_items.append((key, edge_length_by_key[key]))

    angle_sum = 0.0
    if len(path) >= 3:
        for i in range(1, len(path) - 1):
            angle_sum += _turn_angle(graph, path[i - 1], path[i], path[i + 1])

    # Calculate Decision Points (n_nodes)
    # If metadata exists, check if node is a valid decision point OR a target (Entrance/POI)
    # If metadata missing, fallback to len(path) or graph degree logic? 
    # For consistency with previous behavior if file missing, let's use len(path) unless DP_METADATA_FILE was loaded.
    
    if os.path.exists(DP_METADATA_FILE) and valid_decision_points:
        n_decision_points = 0
        for node in path:
            # Check if node is in the validated set OR is an endpoint (target)
            if node in valid_decision_points or ('all_targets_set' in globals() and node in all_targets_set):
                n_decision_points += 1
            # Fallback: if it's the start or end of this specific path, it's a decision point for this trip
            elif node == path[0] or node == path[-1]:
                 n_decision_points += 1
    else:
        # Legacy behavior: count all nodes
        n_decision_points = len(path)

    return {
        "nodes": path,
        "length": length,
        "n_nodes": n_decision_points,
        "angle_sum": angle_sum,
        "edge_items": edge_items,  # list[(edge_key, edge_length_for_key)]
    }

def _sample_x(num_pairs):
    total = num_pairs * (num_pairs + 1) / 2.0
    r = random.random()
    cumulative = 0.0
    for x in range(1, num_pairs + 1):
        cumulative += (num_pairs - x + 1) / total
        if r <= cumulative:
            return x
    return num_pairs

# --- 2. Load or Generate Selected Points ---
selected_points = { "entrances": [], "pois": [] }

if os.path.exists(POINTS_FILE):
    print(f"Loading selected points from {POINTS_FILE}...")
    with open(POINTS_FILE, 'r') as f:
        data = json.load(f)
        selected_points["entrances"] = data.get("entrances", [])
        selected_points["pois"] = data.get("pois", [])
else:
    print(f"'{POINTS_FILE}' not found. Selecting RANDOM points for simulation...")
    all_nodes = list(G.nodes())
    # Pick 2 Entrances and 2 POIs randomly
    random_selection = random.sample(all_nodes, 4)
    selected_points["entrances"] = [{"id": n} for n in random_selection[:2]]
    selected_points["pois"] = [{"id": n} for n in random_selection[2:]]

# Extract Node IDs
def _coerce_node_id(value):
    try:
        return int(value)
    except (TypeError, ValueError):
        return value

entrance_ids = [_coerce_node_id(p['id']) for p in selected_points["entrances"] if p.get('id') is not None]
poi_ids = [_coerce_node_id(p['id']) for p in selected_points["pois"] if p.get('id') is not None]

# Drop IDs not present in the graph (e.g., "unknown" from the UI export)
entrance_ids = [n for n in entrance_ids if n in G]
poi_ids = [n for n in poi_ids if n in G]

# Define Targets (Entrances + POIs)
targets = list(set(entrance_ids + poi_ids))

if len(entrance_ids) == 0 or len(poi_ids) == 0:
    print("Need at least 1 entrance and 1 POI to form entrance->POI pairs.")
    exit()

print(f"Simulation Targets (IDs): {targets}")
all_targets_set = set(targets)

# --- 3. Generate Pairs (z_i) and Importance (kappa_i) ---
# Default scenario generation: pair every entrance (source) with every POI (destination)
pairs = [(s, d) for s, d in itertools.product(entrance_ids, poi_ids) if s != d]
simulation_data = []

# Create lookups for point values (default to 5 if missing, to be safe/consistent with "high importance" default, 
# or 1 if we assume unrated. Given the UI sets 1-5, let's assume default 5 or average 3? 
# The prompt implies values exist. Let's use a helper).
entrance_values = {p['id']: p.get('value', 5) for p in selected_points["entrances"]}
poi_values = {p['id']: p.get('value', 5) for p in selected_points["pois"]}

for s, d in pairs:
    val_s = entrance_values.get(s, 5)
    val_d = poi_values.get(d, 5)
    
    # Calculate importance: (val_s + val_d) / 10
    kappa = (val_s + val_d) / 10.0
    
    # Ensure kappa is within [0, 1] just in case values were out of range
    kappa = max(0.0, min(1.0, kappa))
    
    simulation_data.append({
        "source": s,
        "target": d,
        "kappa": kappa
    })

print(f"Generated {len(simulation_data)} O-D pairs.")

# --- 4. Stage A: Candidate Set Generation via Yen's Algorithm ---
# networkx.shortest_simple_paths yields loopless (simple) paths in increasing weighted length.

print("\n--- Stage A: Candidate Path Generation (Yen) ---")
pair_records = []

for i, data in enumerate(simulation_data):
    s = data["source"]
    t = data["target"]
    kappa = float(data["kappa"])

    print(f"Pair {i+1}: {s} -> {t} (Importance: {kappa:.2f})")

    try:
        path_generator = nx.shortest_simple_paths(G, s, t, weight="length")
        candidates = []

        p1 = next(path_generator)  # may raise StopIteration
        m1 = _path_metrics(G, p1)
        candidates.append(m1)

        threshold = m1["length"] * (1 + THRESHOLD_PERCENT)
        print(f"  - L1: {m1['length']:.2f}m. Threshold: {threshold:.2f}m")

        for pk in path_generator:
            if MAX_CANDIDATES_PER_PAIR is not None and len(candidates) >= MAX_CANDIDATES_PER_PAIR:
                break
            mk = _path_metrics(G, pk)
            if mk["length"] <= threshold:
                candidates.append(mk)
            else:
                break

        print(f"  - Candidates within threshold: {len(candidates)}")

        pair_records.append({
            "pair_index": i,
            "source": s,
            "target": t,
            "kappa": kappa,
            "candidates": candidates,
        })

    except (nx.NetworkXNoPath, nx.NodeNotFound, StopIteration):
        print("  - No path exists between these nodes.")
        pair_records.append({
            "pair_index": i,
            "source": s,
            "target": t,
            "kappa": kappa,
            "candidates": [],
            "no_path": True,
        })

with open("data/simulation_results.json", "w") as f:
    json.dump(pair_records, f, indent=2)

print("Candidate paths saved to data/simulation_results.json")

# --- 5. Stage A: Simulated Annealing Optimization (Choose One Path per Pair) ---
print("\n--- Stage A: Scheme Optimization (Simulated Annealing) ---")

optim_pairs = [p for p in pair_records if p.get("candidates")]
if not optim_pairs:
    print("No pairs with candidates. Cannot optimize scheme.")
    exit()

num_pairs = len(optim_pairs)
num_nodes_total = G.number_of_nodes()

if SA_RANDOM_SEED is not None:
    random.seed(SA_RANDOM_SEED)

def _temperature(iteration):
    return max(SA_TMIN, SA_T0 * (SA_ALPHA ** iteration))

def _scheme_cost(state):
    c_local_length = state["local_length"] / (num_pairs * L_E)
    c_local_nodes = state["local_nodes"] / (num_pairs * num_nodes_total)
    c_local_angle = state["local_angle"] / (num_pairs * num_nodes_total * math.pi)
    c_global_length = state["unique_edge_length"] / L_E
    c_global_nodes = state["unique_node_count"] / num_nodes_total

    total = (
        W_LOCAL_LENGTH * c_local_length
        + W_LOCAL_NODES * c_local_nodes
        + W_LOCAL_ANGLE * c_local_angle
        + W_GLOBAL_LENGTH * c_global_length
        + W_GLOBAL_NODES * c_global_nodes
    )

    return total, {
        "C_L_local": c_local_length,
        "C_N_local": c_local_nodes,
        "C_A_local": c_local_angle,
        "C_L_global": c_global_length,
        "C_N_global": c_global_nodes,
    }

def _add_candidate_to_state(state, kappa, cand):
    state["local_length"] += kappa * cand["length"]
    state["local_nodes"] += kappa * cand["n_nodes"]
    state["local_angle"] += kappa * cand["angle_sum"]

    for node_id in cand["nodes"]:
        prev = state["node_counts"].get(node_id, 0)
        if prev == 0:
            state["unique_node_count"] += 1
        state["node_counts"][node_id] = prev + 1

    for key, key_len in cand["edge_items"]:
        prev = state["edge_counts"].get(key, 0)
        if prev == 0:
            state["unique_edge_length"] += key_len
        state["edge_counts"][key] = prev + 1

def _remove_candidate_from_state(state, kappa, cand):
    state["local_length"] -= kappa * cand["length"]
    state["local_nodes"] -= kappa * cand["n_nodes"]
    state["local_angle"] -= kappa * cand["angle_sum"]

    for node_id in cand["nodes"]:
        prev = state["node_counts"][node_id]
        if prev == 1:
            del state["node_counts"][node_id]
            state["unique_node_count"] -= 1
        else:
            state["node_counts"][node_id] = prev - 1

    for key, key_len in cand["edge_items"]:
        prev = state["edge_counts"][key]
        if prev == 1:
            del state["edge_counts"][key]
            state["unique_edge_length"] -= key_len
        else:
            state["edge_counts"][key] = prev - 1

def _switch_pair_candidate(state, pair_idx, new_cand_idx):
    pair = optim_pairs[pair_idx]
    old_cand_idx = state["selected_indices"][pair_idx]
    if old_cand_idx == new_cand_idx:
        return old_cand_idx

    kappa = pair["kappa"]
    old_cand = pair["candidates"][old_cand_idx]
    new_cand = pair["candidates"][new_cand_idx]

    _remove_candidate_from_state(state, kappa, old_cand)
    _add_candidate_to_state(state, kappa, new_cand)
    state["selected_indices"][pair_idx] = new_cand_idx
    return old_cand_idx

state = {
    "selected_indices": [],
    "local_length": 0.0,
    "local_nodes": 0.0,
    "local_angle": 0.0,
    "edge_counts": {},
    "node_counts": {},
    "unique_edge_length": 0.0,
    "unique_node_count": 0,
}

for p in optim_pairs:
    idx = random.randrange(len(p["candidates"]))
    state["selected_indices"].append(idx)
    _add_candidate_to_state(state, p["kappa"], p["candidates"][idx])

current_cost, current_components = _scheme_cost(state)
best_cost = current_cost
best_components = current_components
best_indices = list(state["selected_indices"])

cost_history = deque([current_cost], maxlen=SA_STOP_WINDOW + 1)

print(f"Initial cost: {current_cost:.6f}")

for it in range(1, SA_MAX_ITERATIONS + 1):
    T = _temperature(it)

    x = _sample_x(num_pairs)
    changed_pairs = random.sample(range(num_pairs), k=min(x, num_pairs))
    applied = []

    for pair_idx in changed_pairs:
        candidates = optim_pairs[pair_idx]["candidates"]
        if len(candidates) <= 1:
            continue
        old_idx = state["selected_indices"][pair_idx]
        new_idx = random.randrange(len(candidates) - 1)
        if new_idx >= old_idx:
            new_idx += 1
        prev_idx = _switch_pair_candidate(state, pair_idx, new_idx)
        applied.append((pair_idx, prev_idx))

    proposed_cost, proposed_components = _scheme_cost(state)
    accept = False

    if proposed_cost <= current_cost:
        accept = True
    else:
        if T > 0:
            exponent = (current_cost - proposed_cost) / T
            accept_prob = 0.0 if exponent < -745 else math.exp(exponent)
            accept = random.random() < accept_prob

    if accept:
        current_cost = proposed_cost
        current_components = proposed_components
        if current_cost < best_cost:
            best_cost = current_cost
            best_components = current_components
            best_indices = list(state["selected_indices"])
    else:
        for pair_idx, prev_idx in reversed(applied):
            _switch_pair_candidate(state, pair_idx, prev_idx)

    cost_history.append(current_cost)

    if it % 1000 == 0:
        print(f"Iter {it}: cost={current_cost:.6f}, best={best_cost:.6f}, T={T:.6g}")

    if len(cost_history) == SA_STOP_WINDOW + 1:
        past = cost_history[0]
        if past != 0 and abs(current_cost - past) / abs(past) < SA_STOP_REL_DELTA:
            print(f"Stopping: <{SA_STOP_REL_DELTA*100:.1f}% cost change over {SA_STOP_WINDOW} iterations.")
            break

scheme_output = {
    "meta": {
        "stage": "A",
        "edge_key_mode": EDGE_KEY_MODE,
        "L_E": L_E,
        "candidate_generation": {
            "threshold_percent": THRESHOLD_PERCENT,
            "max_candidates_per_pair_user_defined": MAX_CANDIDATES_PER_PAIR,
        },
        "weights": {
            "w_local_length": W_LOCAL_LENGTH,
            "w_local_nodes": W_LOCAL_NODES,
            "w_local_angle": W_LOCAL_ANGLE,
            "w_global_length": W_GLOBAL_LENGTH,
            "w_global_nodes": W_GLOBAL_NODES,
        },
        "annealing_user_defined": {
            "random_seed": SA_RANDOM_SEED,
            "max_iterations": SA_MAX_ITERATIONS,
            "stop_window": SA_STOP_WINDOW,
            "stop_rel_delta": SA_STOP_REL_DELTA,
            "T0": SA_T0,
            "alpha": SA_ALPHA,
            "Tmin": SA_TMIN,
        },
        "best_cost": best_cost,
        "best_cost_components": best_components,
    },
    "pairs": [],
}

for pair_idx, pair in enumerate(optim_pairs):
    sel = best_indices[pair_idx]
    cand = pair["candidates"][sel]
    # Candidate 0 is always the shortest path from Yen's algorithm
    shortest_cand = pair["candidates"][0]
    
    scheme_output["pairs"].append({
        "source": pair["source"],
        "target": pair["target"],
        "kappa": pair["kappa"],
        "selected_index": sel,
        "selected_path": cand["nodes"],
        "shortest_path_length": shortest_cand["length"], # Added for visualization comparison
        "selected_metrics": {
            "length": cand["length"],
            "n_nodes": cand["n_nodes"],
            "angle_sum": cand["angle_sum"],
        },
        "num_candidates": len(pair["candidates"]),
    })

with open("data/stageA_scheme.json", "w") as f:
    json.dump(scheme_output, f, indent=2)

print(f"Stage A complete. Scheme saved to data/stageA_scheme.json (pairs optimized: {num_pairs}).")
